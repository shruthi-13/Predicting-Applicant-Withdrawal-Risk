#Using downsampling
# Load Libraries
library(readxl)
library(dplyr)
library(lubridate)
library(caret)
library(C50)
library(partykit)
library(class)
library(e1071)
library(kernlab)
library(nnet)
library(NeuralNetTools)
library(fastDummies)

set.seed(1947)

# Load data
cat("STEP 1: Loading Data...\n")
setwd("C:/Users/Nantika/Downloads/R data files")
tfa_data <- read_xlsx("Group Assignment R.xlsx")
cat("Data loaded:", nrow(tfa_data), "rows\n\n")

# CREATE BALANCED SAMPLE 

cat("STEP 2: Creating Balanced Sample (All Minority + Equal Majority)...\n")
tfa_data <- tfa_data %>% filter(!is.na(`Completed Admissions Process`))

completed <- tfa_data[tfa_data$`Completed Admissions Process` == 1, ]
withdrew <- tfa_data[tfa_data$`Completed Admissions Process` == 0, ]

cat("Original distribution:\n")
cat("  Completed (1):", nrow(completed), "\n")
cat("  Withdrew (0):", nrow(withdrew), "\n\n")

# Identify minority and majority classes
if (nrow(completed) < nrow(withdrew)) {
  minority_class <- completed
  majority_class <- withdrew
  minority_label <- "Completed"
  majority_label <- "Withdrew"
} else {
  minority_class <- withdrew
  majority_class <- completed
  minority_label <- "Withdrew"
  majority_label <- "Completed"
}

cat("Minority class:", minority_label, "with", nrow(minority_class), "samples\n")
cat("Majority class:", majority_label, "with", nrow(majority_class), "samples\n\n")

# Take ALL minority class samples
n_minority <- nrow(minority_class)

# Sample EQUAL number from majority class
set.seed(1947)
sample_majority <- majority_class[sample(nrow(majority_class), n_minority), ]

# Combine datasets
tfa_data <- rbind(minority_class, sample_majority)
tfa_data <- tfa_data[sample(nrow(tfa_data)), ]  # Shuffle

cat("Balanced sample created:\n")
cat("  Total rows:", nrow(tfa_data), "\n")
cat("  Completed (1):", sum(tfa_data$`Completed Admissions Process` == 1), "\n")
cat("  Withdrew (0):", sum(tfa_data$`Completed Admissions Process` == 0), "\n\n")

# FEATURE ENGINEERING
cat("STEP 3: Feature Engineering...\n")

date_cols <- c("Sign-up Date", "Started Date", "Application Deadline", "Submitted Date")
tfa_data[date_cols] <- lapply(tfa_data[date_cols], ymd)

tfa_data <- tfa_data %>%
  mutate(
    days_signup_to_start = as.numeric(difftime(`Started Date`, `Sign-up Date`, units = "days")),
    days_start_to_submit = as.numeric(difftime(`Submitted Date`, `Started Date`, units = "days")),
    days_signup_to_submit = as.numeric(difftime(`Submitted Date`, `Sign-up Date`, units = "days")),
    days_before_deadline = as.numeric(difftime(`Application Deadline`, `Submitted Date`, units = "days")),
    signup_month = month(`Sign-up Date`),
    signup_year = year(`Sign-up Date`),
    submit_month = month(`Submitted Date`),
    submitted_early = ifelse(days_before_deadline > 7, 1, 0),
    quick_starter = ifelse(days_signup_to_start <= 1, 1, 0),
    quick_completer = ifelse(days_start_to_submit <= 7, 1, 0)
  )

# Convert Essays Sentiment to binary factor
tfa_data$`Essays Sentiment` <- ifelse(tfa_data$`Essays Sentiment` >= 0, 0, 
                                      ifelse(tfa_data$`Essays Sentiment` < 0, 1, 
                                             tfa_data$`Essays Sentiment`))
tfa_data$`Essays Sentiment` <- factor(tfa_data$`Essays Sentiment`, 
                                      levels = c(0, 1), 
                                      labels = c("Positive", "Negative"))

tfa_data <- tfa_data %>% select(-all_of(date_cols))
cat("Feature engineering complete\n\n")

# ==================== PREPROCESSING ==================== #
cat("STEP 4: Preprocessing...\n")

high_card_cols <- c("Major 1", "Major 2", "Minor", "Undergraduate University")
for (col in high_card_cols) {
  if (col %in% names(tfa_data)) {
    freq_table <- sort(table(tfa_data[[col]]), decreasing = TRUE)
    top_categories <- names(freq_table)[1:min(20, length(freq_table))]
    tfa_data[[col]] <- ifelse(tfa_data[[col]] %in% top_categories, 
                              tfa_data[[col]], "Other")
  }
}

tfa_data$`Person Id` <- NULL
num_cols <- names(tfa_data)[sapply(tfa_data, is.numeric)]
for (col in num_cols) {
  if (sum(is.na(tfa_data[[col]])) > 0) {
    tfa_data[[col]][is.na(tfa_data[[col]])] <- median(tfa_data[[col]], na.rm = TRUE)
  }
}

cat_cols <- names(tfa_data)[sapply(tfa_data, is.character)]
for (col in cat_cols) {
  if (sum(is.na(tfa_data[[col]])) > 0) {
    mode_val <- names(sort(table(tfa_data[[col]]), decreasing = TRUE))[1]
    tfa_data[[col]][is.na(tfa_data[[col]])] <- mode_val
  }
}

factor_cols <- c(
  "App Started Year (RTAT)",
  "Is Math, Sci, or Eng Major Minor",
  "Major 1",
  "Major 2",
  "Minor",
  "Undergraduate University",
  "School Selectivity",
  "Region Preference Level",
  "Attended Event",
  "Essays Sentiment",
  "submitted_early",
  "quick_starter",
  "quick_completer"
)

# Create dataset for tree-based models (keeps factors)
data_tree <- tfa_data
data_tree[factor_cols] <- lapply(data_tree[factor_cols], as.factor)
data_tree$`Completed Admissions Process` <- factor(
  data_tree$`Completed Admissions Process`,
  levels = c(0, 1),
  labels = c("Withdrew", "Completed")
)

# Create dataset for numeric models (dummy variables)
data_numeric <- dummy_cols(
  tfa_data,
  select_columns = factor_cols,
  remove_first_dummy = TRUE,
  remove_selected_columns = TRUE
)
data_numeric$`Completed Admissions Process` <- factor(
  data_numeric$`Completed Admissions Process`,
  levels = c(0, 1),
  labels = c("Withdrew", "Completed")
)

# Remove near-zero variance
nzv <- nearZeroVar(data_numeric, saveMetrics = FALSE)
if (length(nzv) > 0) {
  data_numeric <- data_numeric[, -nzv]
}

cat("Preprocessing complete\n\n")

# TRAIN-TEST SPLIT 
train_idx_tree <- createDataPartition(data_tree$`Completed Admissions Process`, 
                                      p = 0.8, list = FALSE)
train_tree <- data_tree[train_idx_tree, ]
test_tree <- data_tree[-train_idx_tree, ]

train_idx_num <- createDataPartition(data_numeric$`Completed Admissions Process`, 
                                     p = 0.8, list = FALSE)
train_numeric <- data_numeric[train_idx_num, ]
test_numeric <- data_numeric[-train_idx_num, ]

# Separate X and y for kNN
train_x <- train_numeric %>% select(-`Completed Admissions Process`)
test_x <- test_numeric %>% select(-`Completed Admissions Process`)
train_y <- train_numeric$`Completed Admissions Process`
test_y <- test_numeric$`Completed Admissions Process`

cat("Train:", nrow(train_tree), "| Test:", nrow(test_tree), "\n\n")

# TRAINING CONTROL 
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# MODEL 1: DECISION TREE 
dt_model <- train(
  `Completed Admissions Process` ~ .,
  data = train_tree,
  method = "C5.0",
  trControl = ctrl,
  tuneGrid = expand.grid(
    model = "tree",
    winnow = c(TRUE, FALSE),
    trials = c(1, 10, 20, 30)
  ),
  metric = "ROC"
)

print(dt_model)
cat("\nBest Parameters:\n")
print(dt_model$bestTune)

dt_pred <- predict(dt_model, test_tree)
dt_cm <- confusionMatrix(dt_pred, test_tree$`Completed Admissions Process`, 
                         positive = "Completed")

cat("\n--- DECISION TREE RESULTS ---\n")
print(dt_cm)
cat("\nPrecision:", sprintf("%.4f", dt_cm$byClass['Pos Pred Value']), "\n")

# MODEL 2: k-NEAREST NEIGHBORS 
k_values <- c(3, 5, 7, 11)
best_k <- 5
best_f1 <- 0

for (k in k_values) {
  pred <- knn(train_x, test_x, cl = train_y, k = k)
  cm_temp <- confusionMatrix(pred, test_y, positive = "Completed")
  if (cm_temp$byClass['F1'] > best_f1) {
    best_f1 <- cm_temp$byClass['F1']
    best_k <- k
  }
}

cat("Best k:", best_k, "\n\n")

knn_pred <- knn(train_x, test_x, cl = train_y, k = best_k)
knn_cm <- confusionMatrix(knn_pred, test_y, positive = "Completed")

cat("\n--- k-NEAREST NEIGHBORS RESULTS ---\n")
print(knn_cm)
cat("\nPrecision:", sprintf("%.4f", knn_cm$byClass['Pos Pred Value']), "\n")

# MODEL 3: NAIVE BAYES 

nb_model <- train(
  `Completed Admissions Process` ~ .,
  data = train_tree,
  method = "naive_bayes",
  trControl = ctrl,
  tuneGrid = data.frame(
    laplace = c(0, 1, 2),
    usekernel = TRUE,
    adjust = 1
  ),
  metric = "ROC"
)

print(nb_model)

nb_pred <- predict(nb_model, test_tree)
nb_cm <- confusionMatrix(nb_pred, test_tree$`Completed Admissions Process`, 
                         positive = "Completed")

cat("\n--- NAIVE BAYES RESULTS ---\n")
print(nb_cm)
cat("\nPrecision:", sprintf("%.4f", nb_cm$byClass['Pos Pred Value']), "\n")

#  MODEL 4: SUPPORT VECTOR 
svm_model <- train(
  `Completed Admissions Process` ~ .,
  data = train_numeric,
  method = "svmRadial",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  tuneLength = 3,
  metric = "ROC"
)

print(svm_model)
svm_pred <- predict(svm_model, test_numeric)
svm_cm <- confusionMatrix(svm_pred, test_numeric$`Completed Admissions Process`, 
                          positive = "Completed")

cat("\n--- SUPPORT VECTOR MACHINE RESULTS ---\n")
print(svm_cm)
cat("\nPrecision:", sprintf("%.4f", svm_cm$byClass['Pos Pred Value']), "\n")

# MODEL 5: ARTIFICIAL NEURAL NETWORK 

ann_model <- train(
  `Completed Admissions Process` ~ .,
  data = train_numeric,
  method = "nnet",
  trControl = ctrl,
  preProcess = c("center", "scale"),
  tuneGrid = expand.grid(
    size = c(5, 10),
    decay = c(0.1, 0.3)
  ),
  metric = "ROC",
  maxit = 200,
  trace = FALSE
)

print(ann_model)

ann_pred <- predict(ann_model, test_numeric)
ann_cm <- confusionMatrix(ann_pred, test_numeric$`Completed Admissions Process`, 
                          positive = "Completed")

cat("\n--- ARTIFICIAL NEURAL NETWORK RESULTS ---\n")
print(ann_cm)
cat("\nPrecision:", sprintf("%.4f", ann_cm$byClass['Pos Pred Value']), "\n")


